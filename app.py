import streamlit as st
from ibm_watsonx_ai.foundation_models import ModelInference

# IBM Watsonx model credentials
model_id = "ibm/granite-3-8b-instruct"
project_id = "d87960d2-d01f-44ba-81ef-a16ad656ad73"
credentials = {
    "url": "https://us-south.ml.cloud.ibm.com",
    "apikey": st.secrets["IBM_API_KEY"]
}

# Streamlit UI
st.set_page_config(page_title="EduTutor AI - Explain & MCQ", layout="centered")
st.title("ЁЯУШ EduTutor AI - Explain & Quiz (English / рд╣рд┐рдВрджреА)")
st.markdown("Get an explanation or generate MCQs for any topic using AI.")

# Language selection
language = st.radio("Choose Language / рднрд╛рд╖рд╛ рдЪреБрдиреЗрдВ:", ["English", "Hindi"])
topic = st.text_input("Enter a topic (e.g., Photosynthesis) / рд╡рд┐рд╖рдп рджрд░реНрдЬ рдХрд░реЗрдВ:")
mode = st.radio("Choose what to generate / рдХреНрдпрд╛ рдЬрдирд░реЗрдЯ рдХрд░рдирд╛ рд╣реИ:", ["Topic Explanation", "MCQ Generator"])

if st.button("Generate") and topic.strip():
    # Prompt creation based on language and mode
    if language == "English":
        if mode == "Topic Explanation":
            prompt = f"Explain the topic '{topic}' in detail as if teaching a student."
        else:
            prompt = (
                f"Generate 5 multiple choice questions on the topic '{topic}'. "
                f"Each question should have 4 options (A, B, C, D) and indicate the correct answer at the end."
            )
    else:
        if mode == "Topic Explanation":
            prompt = f"рд╡рд┐рд╖рдп '{topic}' рдХреЛ рдПрдХ рдЫрд╛рддреНрд░ рдХреЛ рдкрдврд╝рд╛рддреЗ рд╕рдордп рд╡рд┐рд╕реНрддрд╛рд░ рд╕реЗ рд╕рдордЭрд╛рдПрдВред"
        else:
            prompt = (
                f"рд╡рд┐рд╖рдп '{topic}' рдкрд░ 5 рдмрд╣реБрд╡рд┐рдХрд▓реНрдкреАрдп рдкреНрд░рд╢реНрди (MCQs) рдмрдирд╛рдПрдВред "
                f"рдкреНрд░рддреНрдпреЗрдХ рдкреНрд░рд╢реНрди рдореЗрдВ 4 рд╡рд┐рдХрд▓реНрдк (A, B, C, D) рд╣реЛрдВ рдФрд░ рдЙрддреНрддрд░ рднреА рд╢рд╛рдорд┐рд▓ рдХрд░реЗрдВред"
            )

    # Model inference
    model = ModelInference(
        model_id=model_id,
        params={"decoding_method": "greedy", "max_new_tokens": 600},
        project_id=project_id,
        credentials=credentials
    )

    # Display results
    with st.spinner("Generating response..."):
        response = model.generate(prompt)
        result_text = response["results"][0]["generated_text"]

    st.markdown("### ЁЯУД Generated Output / рдЙрддреНрдкрдиреНрди рдЙрддреНрддрд░")
    st.markdown(result_text)
